<!DOCTYPE html>
<html lang="en">
	<!--Set up page-->
	<head>
		<meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="stylesheet" href="style.css">
    <title>Aasha's Portfolio</title>
		<style>
			body {
				background-color: #1e2328;
				background-repeat: no-repeat;
				background-size: cover;
				background-position: center center;
				background-attachment: fixed;
			}
		</style>
  </head>

  <!--Navigation bar-->
  <div class="navbar">
			<div class="navbar-right">
      <a href="./index.html">Home</a>
			<a href="./resume.html">Resume</a>
			<a href="./projects.html" class="active">Projects & Research</a>
			<!--<a href="./contact.html">Contact</a>-->
		</div>
</div>

<body class="essay">
  <img src="./images/AI-Discrimination-Header-Image.jpg" alt="silhouettes of a faces" id="header_img">
  <h1>Systemic Discrimination in Facial and Gender Recognition Software</h1>
  <p>If one were to enter the phrase “Black girls” in Google’s search engine 13 years ago,
    text in the immediate results would contain words such as “sugary”, “hairy”, or “porn star”.
    These sexualized results, along with several others, were discovered by informatics scholar
    Dr. Safiya Noble when she conducted this Google search in 2009 (Noble 2018). While shock and
    dismay cloud that finding from over a decade ago, similar emotions are conjured by “color-blind”
    racism that is present in many modern engineered digital systems today. Color-blind systems and
    processes are those that are designed under the illusion that implicit racial biases do not impact
    outputs. Such systems and processes do not take into account instances where racial discrimination
    may be present. By treating race as a simple variable without thoughtful adjustment for societal
    impact, technology stakeholders frequently create irresponsible products that reinforce the
    intersectionality of multiple axes of oppression such as racism and sexism.</p>
  <p>Facial and gender recognition artificial intelligence (AI) is one such example of a color-blind
    technological endeavor. Initial attempts at facial recognition design disproportionately
    mischaracterized features of dark skin individuals. For example, a study by the Massachusetts
    Institute of Technology found that facial recognition software performs worse on darker-skinned women
    (specifically, 18-30 year old Black females) with error rates exceeding 40 percent compared with white
    males (Olson 2021). Implicit bias in the designers leads to products that have those same biases.
    The repercussions of racist design are not often faced by the designers themselves, but by those who
    use the product. Systems like facial recognition being overtly racist holds up societal racism that
    Black people and dark skin people already feel in their lives. To illustrate, Black people account
    for 56% of the United States incarcerated population, according to the NAACP. This translates to an
    overrepresentation of Black people in mug shot data, which is fed into facial recognition technology
    to find accused suspects. Within the last three decades of police using facial recognition systems to
    identify suspects, the case of Robert Julian-Borchak Williams is the first known case of faulty facial
    recognition in which he was arrested for a crime he did not commit based on a false-positive
    identification from the facial recognition system. Williams was taken into custody in front
    of his family with the only evidence against him being a grainy image of a man with the same build (Hill 2020).
    Williams’ false arrest was an irresponsible byproduct of a racially-biased designed recognition system.
    If AI is supposed to be neutral, but the algorithms are racist, then they facilitate oppression in the
    same way society does because it was taught to do so.</p>
    <p>Automatic gender recognition (AGR) software reaffirms the intersection of gender and racial discrimination
      in color-blind technology because AGR and Human-Computer Interaction (HCI) researchers conflating the terms
      of gender and sex contributes to the erasure of transgender people in society. Os Keyes claims that AGR
      research defines gender as “binary, immutable and physiological.” However, gender is a social and cultural
      construct that cannot be defined as a physiological characteristic, which differs from the biological
      definition of sex. One’s body, as defined by their reproductive organs, is not to be essentialized as the
      source of gender; yet, this is an accepted concept in gender informatics. Using AGR software to harness
      gendered spaces such as bathrooms can increase risks of assault, hostility, rejection and steady erasure
      toward transgender and nonbinary people, who are excluded from gender informatics’ definition of gender
      (Keyes 2018).  This automatic gender recognition software includes facial recognition, which, as mentioned
      above, is racist by design. The color-blindness in AGR is the lack of awareness that racially-biased facial
      recognition systems are embedded into identifying gender. The increased harm toward transgender and
      non-binary individuals of color is another irresponsible product of color-blind technologies because the
      harm could easily be reduced if HCI researchers based their research on a more socially inclusive definition
      of gender just as the data that was structured against Williams could have been more representative had it
      not been for the traditional racist views of Black people as criminals.</p>
    <p>If transgender and nonbinary people are becoming more erased from society, then transgender and
      nonbinary people of color are essentially invisible because the intersectionality of their identities
      leads to a deeper structural discrimination. This invisibility is included in the color-blind
      technological structures embedded into the facial and gender recognition systems mentioned above.</p>
    <p>As a dark-skinned, feminine-identifying person of color who is passionate about pursuing progress in AI systems,
      I have come to recognize the discrimination I have experienced as a result of these color-blind and gender-biased
      AI technologies, and this deeply saddens me. Although society has become more technologically advanced, the
      technology has not advanced racially because color-blind racism has become embedded into the culture of technology
      development. For instance, it leads me to question if Apple has failed to unlock the devices of my white counterparts
      when using facial recognition software in dark settings as it has failed me. It leads me to wonder how I can become
      visible in a growing technological field that is blind toward me. One impactful way to increase visibility of
      dark-skinned people of different genders would be to include them in the design process as managers, designers,
      researchers and study participants. Our input and experiences are essential to recognizing that color-blindness
      in facial and gender recognition software is counterproductive to minimizing the intersectional oppression of
      racism and sexism.</p>
		<h2>Works Cited</h2>
		<ul>
		  <li>Benjamin, R. (2019). “Chapter 2.” Default Discrimination. Race after technology. Cambridge, UK: Polity.</li>
		  <li>Buolamwini, J., & Gebru, T. (2018). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender
		    Classification. Proceedings of Machine Learning Research 81 .<a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">
		      http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf</a></li>
		  <li>Criminal Justice Fact Sheet. (2021, May 24). NAACP. Retrieved October 17, 2022, from <a href="https://naacp.org/resources/criminal-justice-fact-sheet">
		    https://naacp.org/resources/criminal-justice-fact-sheet</li>
		  <li>Daniels, J. (2015). My Brain Database Doesn’t See Skin Color: Color-Blind Racism in the Technology Industry
		    and in Theorizing the Web. American Behavioral Scientist, 59(11), 1377–1393.</li>
		  <li>Hill, K. (2020, August 3). Wrongfully accused by an algorithm (Published 2020). The New York Times -
		    Breaking News, US News, World News and Videos. <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">
		      https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</a></li>
		  <li>Keyes, O. (2018). The misgendering machines: Trans/HCI implications of automatic gender recognition.
		    Proceedings of the ACM on human-computer interaction, 2(CSCW), 1-22.</li>
		  <li>Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. NYU Press.</li>
		  <li>Olson, A. (2021, December 9). Facial Recognition Tech Perpetuates Racial Bias. So Why Are We Still Using It?
		    YES! Magazine. Retrieved October 17, 2022, from <a href="https://www.yesmagazine.org/opinion/2021/12/09/facial-recognition-software-racial-bias">
		      https://www.yesmagazine.org/opinion/2021/12/09/facial-recognition-software-racial-bias</a></li>
		</ul>
</body>

</html>
